{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf6758b-889f-40e5-af8a-8ef401ae3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/kalen/Desktop/Python_env/my_paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b559be-cb15-467e-82a5-05e8a6a6da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Type\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54981c0d-65c1-4e51-bfa0-c383641b1a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# 一 图像编码\n",
    "from ViT_Encoder import ImageReader,PatchEmbed, ImageEncoderViT\n",
    "\n",
    "# 1 简单的编码测试\n",
    "# (1) 读取数据测试\n",
    "image_reader = ImageReader(size=(256, 256)) # 初始化定义将读取的图片转为shape = [256, 256]\n",
    "image = '/Users/kalen/Desktop/Python_env/my_paper/cat.jpg'\n",
    "output_from_image_reader = image_reader(image)\n",
    "print(output_from_image_reader.shape) # shape = [1, 3, 256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11731fa-5dc6-46c8-8b1f-b2425a2df04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# (2) 图片编码测试（已经将ImageEmbed类打包进了EImageEncoderViT里）\n",
    "image_encoder = ImageEncoderViT()\n",
    "output_from_untrained_ImageEncoderViT = image_encoder(output_from_image_reader)\n",
    "print(output_from_untrained_ImageEncoderViT.shape) # shape = [1, 256, 256, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67c949-9110-4aa3-8e58-bf7a9a6a4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 读取chackpoint测试\n",
    "# (1) 读取checkpoint\n",
    "checkpoint = '/Users/kalen/Desktop/Python_env/my_paper/sam_vit_b_01ec64.pth'\n",
    "checkpoint = torch.load(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db570a-e734-4068-892b-ceb457dfd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 实例化model，并读取checkpoint\n",
    "model = ImageEncoderViT()\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d976e06-f985-4af0-89ad-e1f9252fcd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# (3) 测试图像数据\n",
    "output_from_checkpoint_ImageEncoderViT = model(output_from_image_reader)\n",
    "print(output_from_checkpoint_ImageEncoderViT.shape) # shape = [1, 256, 256, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b8c70-5bb8-4780-b658-9749b03aa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二 自然语言编码\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 编码工作流程: 自然语言描述 -> BertTokenizer -> tensor向量 -> BertModel -> 输出\n",
    "\n",
    "# 1 定义自然语言描述\n",
    "text = 'I want to work hard to get the CVPR 2025'\n",
    "\n",
    "# 2 加载tokenizer和BERT模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ddc396f-dbdc-4a43-9925-8fbe30d90581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_from_tokenizer的输出为: {'input_ids': tensor([[  101,  1045,  2215,  2000,  2147,  2524,  2000,  2131,  1996, 26226,\n",
      "         18098, 16798,  2629,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "output_from_bert_model的last_hidden_state输出为: torch.Size([1, 14, 768])\n",
      "output_from_bert_model的poller_output输出为: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# 3 用tokenizer和bert对自然语言编码\n",
    "output_from_tokenizer = tokenizer(text, return_tensors=\"pt\") #  return_tensors=\"pt\"参数表示希望输出的是torch.Tensor\n",
    "print(\"output_from_tokenizer的输出为:\", output_from_tokenizer) # 但实际输出的是一个3个元素的字典，形如{'input_ids: tensor, 'token_type_ids': tensor, 'attention_mask': tensor}\n",
    "\n",
    "output_from_bert_model = bert_model(**output_from_tokenizer) # 由于output_from_tokenizer是个字典，所以前面要加两个星号\n",
    "'''bert_model(**output_from_tokenizer)的写法，等效于model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], token_type_ids=inputs.get(\"token_type_ids\"))'''\n",
    "\n",
    "# output_from_bert_model其实也是个字典，即{'last_hidden_state': tensor, 'pooler_output': tensor}\n",
    "print(\"output_from_bert_model的last_hidden_state输出为:\", output_from_bert_model.last_hidden_state.shape)\n",
    "print(\"output_from_bert_model的poller_output输出为:\", output_from_bert_model.pooler_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
